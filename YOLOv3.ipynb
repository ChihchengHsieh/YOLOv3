{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from __future__ import division\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np \n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as T\n",
    "from model import Model_defs\n",
    "from collections import defaultdict\n",
    "from coco_dict import COCO_Dict\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import torch.utils.data as data\n",
    "from PIL import Image\n",
    "import os.path\n",
    "\n",
    "\n",
    "to_img= T.Compose([T.ToPILImage()])\n",
    "to_tensor = T.Compose([T.ToTensor()])\n",
    "#load_norm = T.Compose([T.Resize((hps.image_size,hps.image_size)),T.ToTensor(),T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "device = torch.device('gpu' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "'''\n",
    "TODO:\n",
    "\n",
    "1. Add Pixelshuffle\n",
    "2. Add Self.attention\n",
    "3. Test the model on GPU\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.makedirs('./data/coco')\n",
    "# os.makedirs('./data/coco/anno')\n",
    "dict = {1:'apple',96:'water'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser():\n",
    "    #hyperparameters\n",
    "    def __init__(self):\n",
    "        self.img_size = 416\n",
    "        self.img_path = './data/coco/val2017' # folder\n",
    "        self.ann_path = './data/coco/annotations/instances_val2017.json' # file name\n",
    "        self.img_channels = 3\n",
    "        self.ignore_thres = 0.5\n",
    "        self.lambda_coord = 1\n",
    "        self.batch_size = 5\n",
    "        self.num_anchors = 3\n",
    "        self.num_classes = 80\n",
    "        self.max_objs = 50\n",
    "        self.model_name = 'YOLOv3'\n",
    "        self.model_path = './'+ self.model_name +'/Model/'\n",
    "        self.img_save_path = './'+self.model_name+'/Image/'\n",
    "        self.lr = 0.02\n",
    "        self.beta1 = 0.9\n",
    "        self.beta2 = 0.999\n",
    "        self.n_epoch = 30\n",
    "        self.loss_show_freq = 200\n",
    "        self.img_show_freq = 200\n",
    "        self.model_save_freq = 1000\n",
    "        \n",
    "args = Parser()\n",
    "\n",
    "class Upsample(nn.Module):\n",
    "    def __init__(self, size = None, scale_factor = 2, mode ='nearest', align_corners = None):\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "        self.scale_factor = scale_factor\n",
    "        self.mode = mode\n",
    "        self.align_corners = align_corners\n",
    "    def forward(self, x):\n",
    "        return F.interpolate(x, self.size, self.scale_factor, self.mode, self.align_corners)\n",
    "\n",
    "\n",
    "if not os.path.exists(args.model_path):\n",
    "    os.makedirs(args.model_path)\n",
    "if not os.path.exists(args.img_save_path):\n",
    "    os.makedirs(args.img_save_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class COCODataset(Dataset):\n",
    "\n",
    "    def __init__(self, root, annFile, transform=None, target_transform=None):\n",
    "        from pycocotools.coco import COCO\n",
    "        self.root = root\n",
    "        self.coco = COCO(annFile)\n",
    "        self.ids = list(self.coco.imgs.keys())\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.resize = T.Compose([T.ToPILImage(),T.Resize((args.img_size,args.img_size)),T.ToTensor()])\n",
    "        self.max_objs = args.max_objs\n",
    "        \n",
    "        # Map from 1~90 to 0~79\n",
    "        self.ninety_to_eighty = list(np.arange(0,80))\n",
    "        for i in [0,12,26,29,30,45,66,68,69,71,83]:\n",
    "            self.ninety_to_eighty.insert(i,None)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        coco = self.coco\n",
    "        img_id = self.ids[index]\n",
    "        ann_ids = coco.getAnnIds(imgIds=img_id)\n",
    "        anno = coco.loadAnns(ann_ids)\n",
    "\n",
    "        path = coco.loadImgs(img_id)[0]['file_name']\n",
    "\n",
    "        img = Image.open(os.path.join(self.root, path)).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(anno)\n",
    "        \n",
    "        _, h, w = img.shape\n",
    "        dim_diff = abs(h-w)\n",
    "        pad1, pad2 = dim_diff // 2, dim_diff - dim_diff // 2\n",
    "        pad = (0,0,pad1, pad2,0,0) if h <= w else (pad1, pad2,0,0,0,0)\n",
    "        img = F.pad(img, pad, mode='constant', value=0.5)\n",
    "        _, padded_h, padded_w = img.shape\n",
    "        img = self.resize(img)\n",
    "        \n",
    "        bbox = torch.tensor([i['bbox'] for i in anno]) # x,y,w,h\n",
    "        cat = [self.ninety_to_eighty[i['category_id']] for i in anno]\n",
    "        if (len(cat) == 0):\n",
    "            return self.__getitem__(torch.randint(len(self),(1,1)).int().item())\n",
    "        category_id = torch.LongTensor(cat).float().unsqueeze(1)\n",
    "\n",
    "        labels = torch.cat([category_id, bbox], dim = 1)\n",
    "            \n",
    "            \n",
    "        labels[:,1] = labels[:,1] + pad[0]\n",
    "        labels[:,2] = labels[:,2] + pad[3]\n",
    "        \n",
    "        labels[:,1:] *= (args.img_size/ padded_h)\n",
    "        \n",
    "        labels[:,1] += labels[:,3]/2\n",
    "        labels[:,2] += labels[:,4]/2\n",
    "        \n",
    "        filled_labels = torch.zeros((self.max_objs, 5))\n",
    "        if labels is not None:\n",
    "            filled_labels[range(len(labels))[:self.max_objs]] = labels[:self.max_objs]\n",
    "        \n",
    "        # the output will become scaled Class, CenterX, CenterY, Gw, Gh\n",
    "        return  path , img, filled_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "\n",
    "    def __init__(self, root, transform = None):\n",
    "        from glob import glob\n",
    "        self.root = root \n",
    "        self.files = sorted(glob(os.path.join(root, '*.jpg')))\n",
    "        self.transform = transform\n",
    "        self.resize = T.Compose([T.ToPILImage(),T.Resize((args.img_size,args.img_size)),T.ToTensor()])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        path = self.files[index % len(self.files)]\n",
    "        img = Image.open(path)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        _, h, w = img.shape\n",
    "        dim_diff = abs(h-w)\n",
    "        pad1, pad2 = dim_diff // 2, dim_diff - dim_diff // 2\n",
    "        pad = (0,0,pad1, pad2,0,0) if h <= w else (pad1, pad2,0,0,0,0)\n",
    "        img = F.pad(img, pad, mode='constant', value=0.5)\n",
    "        _, padded_h, padded_w = img.shape\n",
    "        img = self.resize(img)\n",
    "        path = os.path.relpath(path, start= self.root)\n",
    "        return  path, img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbox_iou(box1, box2, x1y1x2y2=False):\n",
    "    \"\"\"\n",
    "    Returns the IoU of two bounding boxes\n",
    "    \"\"\"\n",
    "    if not x1y1x2y2:\n",
    "        # Transform from center and width to exact coordinates\n",
    "        b1_x1, b1_x2 = box1[:, 0] - box1[:, 2] / 2, box1[:, 0] + box1[:, 2] / 2\n",
    "        b1_y1, b1_y2 = box1[:, 1] - box1[:, 3] / 2, box1[:, 1] + box1[:, 3] / 2\n",
    "        b2_x1, b2_x2 = box2[:, 0] - box2[:, 2] / 2, box2[:, 0] + box2[:, 2] / 2\n",
    "        b2_y1, b2_y2 = box2[:, 1] - box2[:, 3] / 2, box2[:, 1] + box2[:, 3] / 2\n",
    "    else:\n",
    "        # Get the coordinates of bounding boxes\n",
    "        b1_x1, b1_y1, b1_x2, b1_y2 = box1[:,0], box1[:,1], box1[:,2], box1[:,3]\n",
    "        b2_x1, b2_y1, b2_x2, b2_y2 = box2[:,0], box2[:,1], box2[:,2], box2[:,3]\n",
    "\n",
    "    # get the corrdinates of the intersection rectangle\n",
    "    inter_rect_x1 =  torch.max(b1_x1, b2_x1)\n",
    "    inter_rect_y1 =  torch.max(b1_y1, b2_y1)\n",
    "    inter_rect_x2 =  torch.min(b1_x2, b2_x2)\n",
    "    inter_rect_y2 =  torch.min(b1_y2, b2_y2)\n",
    "    # Intersection area\n",
    "    inter_area =    torch.clamp(inter_rect_x2 - inter_rect_x1 + 1, min=0) * \\\n",
    "                    torch.clamp(inter_rect_y2 - inter_rect_y1 + 1, min=0)\n",
    "    # Union Area\n",
    "    b1_area = (b1_x2 - b1_x1 + 1) * (b1_y2 - b1_y1 + 1)\n",
    "    b2_area = (b2_x2 - b2_x1 + 1) * (b2_y2 - b2_y1 + 1)\n",
    "\n",
    "    iou = inter_area / (b1_area + b2_area - inter_area + 1e-16)\n",
    "\n",
    "    return iou\n",
    "\n",
    "\n",
    "def non_max_suppression(prediction, num_classes, conf_thres=0.5, nms_thres=0.4):\n",
    "    \"\"\"\n",
    "    Removes detections with lower object confidence score than 'conf_thres' and performs\n",
    "    Non-Maximum Suppression to further filter detections.\n",
    "    Returns detections with shape:\n",
    "        (x, y, w, h, object_conf, class_score, class_pred)\n",
    "    \"\"\"\n",
    "    \n",
    "    output = [None for _ in range(len(prediction))] # len = args.batch_size\n",
    "    for image_i, image_pred in enumerate(prediction):\n",
    "        # Filter out confidence scores below threshold\n",
    "        conf_mask = (image_pred[:, 4] >= conf_thres).squeeze()\n",
    "        image_pred = image_pred[conf_mask]\n",
    "        # If none are remaining => process next image\n",
    "        if not image_pred.size(0):\n",
    "            continue\n",
    "        # Get score and class with highest confidence\n",
    "        class_conf, class_pred = torch.max(image_pred[:, 5:5 + num_classes], 1,  keepdim=True)\n",
    "        # Detections ordered as (x1, y1, x2, y2, obj_conf, class_conf, class_pred)\n",
    "        detections = torch.cat((image_pred[:, :5], class_conf.float(), class_pred.float()), 1)\n",
    "        # Iterate through all predicted classes\n",
    "        unique_labels = detections[:, -1].cpu().unique()\n",
    "        if prediction.is_cuda:\n",
    "            unique_labels = unique_labels.cuda()\n",
    "        for c in unique_labels: # certain class\n",
    "            # Get the detections with the particular class\n",
    "            detections_class = detections[detections[:, -1] == c]\n",
    "            # Sort the detections by maximum objectness confidence\n",
    "            \n",
    "            \n",
    "            _, conf_sort_index = torch.sort(detections_class[:, 4], descending=True)\n",
    "            detections_class = detections_class[conf_sort_index]\n",
    "            # Perform non-maximum suppression\n",
    "            max_detections = []\n",
    "            while detections_class.size(0):\n",
    "                # Get detection with highest confidence and save as max detection\n",
    "                max_detections.append(detections_class[0].unsqueeze(0))\n",
    "                # Stop if we're at the last detection\n",
    "                if len(detections_class) == 1:\n",
    "                    break\n",
    "                # Get the IOUs for all boxes with lower confidence\n",
    "                ious = bbox_iou(max_detections[-1], detections_class[1:])\n",
    "                # Remove detections with IoU >= NMS threshold\n",
    "                detections_class = detections_class[1:][ious < nms_thres]\n",
    "\n",
    "            max_detections = torch.cat(max_detections).data\n",
    "            # add max detection to the certain place of the list\n",
    "            output[image_i] = max_detections if output[image_i] is None else torch.cat((output[image_i], max_detections))\n",
    "\n",
    "    return output\n",
    "\n",
    "def plot_on_o_img(path, input_labels, input_classes, save = False, step ='Train'):\n",
    "    '''\n",
    "    input labels should be x,y,w,h\n",
    "    '''\n",
    "    labels = input_labels.clone()\n",
    "    classes = input_classes.clone()\n",
    "    cmap = plt.get_cmap('tab20b')\n",
    "    colors = [cmap(i) for i in np.linspace(0, 1, args.num_classes)]\n",
    "    \n",
    "    for p, label, cls in zip(path, labels, classes):\n",
    "        s_img_p = os.path.join(args.img_path, p)\n",
    "        img = to_tensor(Image.open(s_img_p))\n",
    "        \n",
    "        plt.figure()\n",
    "        fig, ax = plt.subplots(figsize= (8,8))\n",
    "        ax.imshow(to_img(img))\n",
    "        \n",
    "        pad_x = max(img.shape[1] - img.shape[2], 0)//2\n",
    "        pad_y = max(img.shape[2] - img.shape[1], 0)//2\n",
    "        \n",
    "        idx = torch.any(label[...,2:4].byte(),-1)\n",
    "        label = label[idx]\n",
    "        \n",
    "        if not label.size(0):\n",
    "            print('None Object Detected')\n",
    "            plt.show()\n",
    "            continue\n",
    "            \n",
    "        label[:,0] -= label[:,2]/2\n",
    "        label[:,1] -= label[:,3]/2\n",
    "        label[:,:] *= (max(img.shape[1:])/args.img_size)\n",
    "        label[:,0] -= pad_x\n",
    "        label[:,1] -= pad_y\n",
    "             \n",
    "        \n",
    "\n",
    "        for lab, c in zip(label, cls):\n",
    "            x1,y1,w,h = lab\n",
    "            bbox = patches.Rectangle((x1, y1), w, h, linewidth=2,\n",
    "                                edgecolor= colors[int(c)],facecolor='none')\n",
    "            \n",
    "            ax.add_patch(bbox)\n",
    "            plt.text(x1, y1, s=str(COCO_Dict.id_to_cat[int(c)]), color='white', verticalalignment='top',\n",
    "                            bbox={'color': colors[int(c)], 'pad': 0})\n",
    "        plt.show()\n",
    "            \n",
    "        if save:\n",
    "            fig.savefig(args.img_save_path + args.model_name + p[:-4] +\"_pred_\"+str(step)+\".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_targets(pred_boxes, target, anchors, num_anchors, num_classes, dim, ignore_thres, img_dim):\n",
    "    '''\n",
    "    Based on 416x416 coordinate.\n",
    "    '''\n",
    "    nB = target.size(0)\n",
    "    nA = num_anchors\n",
    "    nC = num_classes\n",
    "    dim = dim\n",
    "    n_grid = dim \n",
    "    grid_size = args.img_size / n_grid \n",
    "    # The feature map is (B, num_anchors, g_dim, g_dim, self.bbox_attrs) Pred_boxes\n",
    "\n",
    "    mask        = torch.zeros(nB, nA, dim, dim)\n",
    "    conf_mask   = torch.ones(nB, nA, dim, dim)\n",
    "    tx          = torch.zeros(nB, nA, dim, dim)\n",
    "    ty          = torch.zeros(nB, nA, dim, dim)\n",
    "    tw          = torch.zeros(nB, nA, dim, dim)\n",
    "    th          = torch.zeros(nB, nA, dim, dim)\n",
    "    tconf       = torch.zeros(nB, nA, dim, dim)\n",
    "    tcls        = torch.zeros(nB, nA, dim, dim, num_classes)\n",
    "\n",
    "    nGT = 0 # number of ground truth\n",
    "    nCorrect = 0\n",
    "    # 將target轉換為Feature map\n",
    "    for b in range(nB): # every batch\n",
    "        for t in range(target.shape[1]): # every bbox from target \n",
    "            if target[b, t].sum() == 0:\n",
    "                continue\n",
    "            nGT += 1\n",
    "            # Convert to position relative to box\n",
    "            gx = target[b, t, 1]\n",
    "            gy = target[b, t, 2]\n",
    "            gw = target[b, t, 3]\n",
    "            gh = target[b, t, 4]\n",
    "            # Get grid box indices\n",
    "            gi = (gx/ grid_size).long()\n",
    "            gj = (gy/ grid_size).long()\n",
    "\n",
    "            # Get shape of gt box\n",
    "            gt_box = torch.tensor([0,0,gw,gh]).unsqueeze(0)\n",
    "            # Get shape of anchor box\n",
    "            # input is scaled anchors\n",
    "            anchor_shapes = torch.cat([torch.zeros((len(anchors),2)),torch.tensor(anchors).float()], dim =1 )\n",
    "            # Calculate iou between gt and anchor shapes\n",
    "            anch_ious = bbox_iou(gt_box, anchor_shapes, False)\n",
    "            # Where the overlap is larger than threshold set mask to zero (ignore)\n",
    "            conf_mask[b, anch_ious > args.ignore_thres] = 0\n",
    "            # Find the best matching anchor box\n",
    "            best_n = np.argmax(anch_ious)\n",
    "            # Get ground truth box\n",
    "            gt_box = torch.FloatTensor(np.array([gx, gy, gw, gh])).unsqueeze(0)\n",
    "            # Get the best prediction\n",
    "            pred_box = pred_boxes[b, best_n, gj, gi].unsqueeze(0)\n",
    "            # Masks\n",
    "            mask[b, best_n, gj, gi] = 1\n",
    "            conf_mask[b, best_n, gj, gi] = 1\n",
    "            tx[b, best_n, gj, gi] = gx\n",
    "            ty[b, best_n, gj, gi] = gy\n",
    "            tw[b, best_n, gj, gi] = gw\n",
    "            th[b, best_n, gj, gi] = gh\n",
    "            # One-hot encoding of label\n",
    "            tcls[b, best_n, gj, gi, int(target[b, t, 0])] = 1\n",
    "            # Calculate iou between ground truth and best matching prediction\n",
    "            iou = bbox_iou(gt_box, pred_box, x1y1x2y2=False)\n",
    "            tconf[b, best_n, gj, gi] = 1\n",
    "\n",
    "            if iou > 0.5:\n",
    "                nCorrect += 1\n",
    "    \n",
    "    t = torch.stack([tx,ty,tw,th],-1)\n",
    "    return nGT, nCorrect, mask, conf_mask, t, tconf, tcls\n",
    "\n",
    "def create_modules(module_defs):\n",
    "    \"\"\"\n",
    "    Constructs module list of layer blocks from module configuration in module_defs\n",
    "    \"\"\"\n",
    "    output_filters = [args.img_channels]\n",
    "    module_list = nn.ModuleList()\n",
    "    for i, module_def in enumerate(module_defs):\n",
    "        modules = nn.Sequential()\n",
    "        # same_pad = lambda k, s: ((k - 1) * s + 1) // 2 ## Should I use this in padding?\n",
    "        if module_def['type'] == 'Conv':\n",
    "            filters = int(module_def['n_fiter'])\n",
    "            kernel_size = int(module_def['kernel'])\n",
    "            pad = (kernel_size - 1) // 2 if int(module_def['pad']) else 0\n",
    "            modules.add_module('conv_%d' % i, nn.Conv2d(in_channels=output_filters[-1],\n",
    "                                                        out_channels=filters,\n",
    "                                                        kernel_size=kernel_size,\n",
    "                                                        stride=int(module_def['stride']),\n",
    "                                                        padding=pad,\n",
    "                                                        bias=not module_def['bn']))\n",
    "            if module_def['bn']:\n",
    "                modules.add_module('batch_norm_%d' % i, nn.BatchNorm2d(filters))\n",
    "            if module_def['act'] == 'leaky':\n",
    "                modules.add_module('leaky_%d' % i, nn.LeakyReLU(0.1))\n",
    "\n",
    "        elif module_def['type'] == 'upsample':\n",
    "            upsample = Upsample( scale_factor=int(module_def['stride']),\n",
    "                                    mode='nearest')\n",
    "            modules.add_module('upsample_%d' % i, upsample)\n",
    "\n",
    "        elif module_def['type'] == 'route':\n",
    "            layers = module_def[\"layers\"]\n",
    "            filters = sum([output_filters[layer_i] for layer_i in layers])\n",
    "            modules.add_module('route_%d' % i, EmptyLayer())\n",
    "\n",
    "        elif module_def['type'] == 'shortcut':\n",
    "            filters = output_filters[int(module_def['from'])]\n",
    "            modules.add_module(\"shortcut_%d\" % i, EmptyLayer())\n",
    "\n",
    "        elif module_def[\"type\"] == \"yolo\":\n",
    "            anchor_idxs = module_def[\"mask\"]\n",
    "            # Extract anchors\n",
    "            anchors = module_def[\"anchors\"]\n",
    "            anchors = [(anchors[i], anchors[i+1]) for i in range(0, len(anchors),2)]\n",
    "            anchors = [anchors[i] for i in anchor_idxs]\n",
    "            # Define detection layer\n",
    "            yolo_layer = YOLOLayer(anchors)\n",
    "            modules.add_module('yolo_%d' % i, yolo_layer)\n",
    "        # Register module list and number of output filters\n",
    "        module_list.append(modules)\n",
    "        output_filters.append(filters)\n",
    "\n",
    "    return module_list\n",
    "\n",
    "class EmptyLayer(nn.Module):\n",
    "    \"\"\"Placeholder for 'route' and 'shortcut' layers\"\"\"\n",
    "    def __init__(self):\n",
    "        super(EmptyLayer, self).__init__()\n",
    "\n",
    "class YOLOLayer(nn.Module):\n",
    "    \"\"\"Detection layer\"\"\"\n",
    "    def __init__(self, anchors):\n",
    "        super(YOLOLayer, self).__init__()\n",
    "        self.anchors = anchors\n",
    "        self.num_anchors = args.num_anchors\n",
    "        self.num_classes = args.num_classes\n",
    "        self.bbox_attrs = 5 + args.num_classes\n",
    "        self.ignore_thres = args.ignore_thres\n",
    "        self.lambda_coord = args.lambda_coord\n",
    "\n",
    "        self.mse_loss = nn.MSELoss().to(device)\n",
    "        self.bce_loss = nn.BCELoss().to(device)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        \n",
    "        '''\n",
    "        Based on 416x416 coordinate\n",
    "        '''\n",
    "        bs = x.size(0)\n",
    "        n_grids = x.size(2) # n_grids\n",
    "        grid_size = args.img_size / n_grids \n",
    "        # Tensors for cuda support\n",
    "        FloatTensor = torch.cuda.FloatTensor if x.is_cuda else torch.FloatTensor\n",
    "        LongTensor = torch.cuda.LongTensor if x.is_cuda else torch.LongTensor\n",
    "\n",
    "        prediction = x.view(bs, self.num_anchors, self.bbox_attrs, n_grids, n_grids).permute(0, 1, 3, 4, 2).contiguous()\n",
    "        # (B, num_anchors, g_dim, g_dim, self.bbox_attrs)\n",
    "        # Get outputs\n",
    "        x = torch.sigmoid(prediction[..., 0])          # Center x\n",
    "        y = torch.sigmoid(prediction[..., 1])          # Center y\n",
    "        w = prediction[..., 2]                         # Width %\n",
    "        h = prediction[..., 3]                         # Height %\n",
    "        conf = torch.sigmoid(prediction[..., 4])       # Conf\n",
    "        pred_cls = torch.sigmoid(prediction[..., 5:])  # Cls pred.\n",
    "\n",
    "        # Calculate offsets for each grid\n",
    "        \n",
    "        grid = torch.arange(n_grids)\n",
    "        grid_y,grid_x = torch.meshgrid((grid,grid))\n",
    "        grid_x = grid_x.repeat(bs*args.num_anchors, 1, 1).view(x.shape).float()\n",
    "        grid_y = grid_y.repeat(bs*args.num_anchors, 1, 1).view(y.shape).float()\n",
    "\n",
    "        scaled_anchors = self.anchors\n",
    "        anchor_w = FloatTensor(self.anchors).index_select(1, LongTensor([0]))\n",
    "        anchor_h = FloatTensor(self.anchors).index_select(1, LongTensor([1]))\n",
    "        anchor_w = anchor_w.repeat(bs, 1).repeat(1, 1, n_grids*n_grids).view(w.shape)\n",
    "        anchor_h = anchor_h.repeat(bs, 1).repeat(1, 1, n_grids*n_grids).view(h.shape)\n",
    "\n",
    "        # Add offset and scale with anchors\n",
    "        pred_boxes = torch.zeros_like(prediction[..., :4]) \n",
    "        pred_boxes[..., 0] = (x.data + grid_x) * grid_size\n",
    "        pred_boxes[..., 1] = (y.data + grid_y) * grid_size\n",
    "        pred_boxes[..., 2] = torch.exp(w.data) * anchor_w\n",
    "        pred_boxes[..., 3] = torch.exp(h.data) * anchor_h\n",
    "        \n",
    "        self.output = torch.cat((pred_boxes.view(bs, -1, 4) , conf.view(bs, -1, 1), pred_cls.view(bs, -1, args.num_classes)), -1).detach()\n",
    "        # Training\n",
    "        if targets is not None:\n",
    "\n",
    "            nGT, nCorrect, mask, conf_mask, t, tconf, tcls = build_targets(pred_boxes.cpu().data,\n",
    "                                                                            targets.cpu().data,\n",
    "                                                                            self.anchors,\n",
    "                                                                            args.num_anchors,\n",
    "                                                                            args.num_classes,\n",
    "                                                                            n_grids,\n",
    "                                                                            args.ignore_thres,\n",
    "                                                                            args.img_size)\n",
    "            \n",
    "            \n",
    "            nProposals = int((conf > 0.25).sum().item())\n",
    "            recall = float(nCorrect / nGT) if nGT else 1\n",
    "            mask = mask.float()\n",
    "\n",
    "            \n",
    "            t[...,2:] = t[...,2:].sqrt()\n",
    "            pred_boxes[...,2:] = pred_boxes[...,2:].sqrt()\n",
    "            \n",
    "            coord_loss = torch.mean(((t - pred_boxes).pow(2))* mask.unsqueeze(-1).float())\n",
    "            \n",
    "            cls_mask = mask.unsqueeze(-1).repeat(1, 1, 1, 1, args.num_classes)\n",
    "            conf_mask = conf_mask.float()\n",
    "            loss_conf = self.bce_loss(conf * conf_mask, tconf * conf_mask)\n",
    "            loss_cls = self.bce_loss(pred_cls * cls_mask, tcls * cls_mask)\n",
    "            loss = coord_loss + loss_conf + loss_cls\n",
    "\n",
    "            return loss, coord_loss.item(), loss_conf.item(), loss_cls.item(), recall\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "\n",
    "class Darknet(nn.Module):\n",
    "    \"\"\"YOLOv3 object detection model\"\"\"\n",
    "    def __init__(self, module_defs):\n",
    "        super(Darknet, self).__init__()\n",
    "        self.module_defs = module_defs\n",
    "        self.module_list = create_modules(self.module_defs)\n",
    "        self.img_size = args.img_size\n",
    "        self.seen = 0\n",
    "        self.header_info = np.array([0, 0, 0, self.seen, 0])\n",
    "        self.loss_names = ['coord', 'conf', 'cls', 'recall']\n",
    "        self.train_hist = defaultdict(list)\n",
    "        self.apply(self.weight_init)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        is_training = targets is not None\n",
    "        output = []\n",
    "        self.pred = []\n",
    "        self.losses = defaultdict(float)\n",
    "        layer_outputs = []\n",
    "        for i, (module_def, module) in enumerate(zip(self.module_defs, self.module_list)):\n",
    "            if module_def['type'] in ['Conv', 'upsample']:\n",
    "                x = module(x)\n",
    "            elif module_def['type'] == 'route':\n",
    "                layer_i = module_def['layers']\n",
    "                x = torch.cat([layer_outputs[i] for i in layer_i], 1)\n",
    "            elif module_def['type'] == 'shortcut':\n",
    "                layer_i = int(module_def['from'])\n",
    "                x = layer_outputs[-1] + layer_outputs[layer_i]\n",
    "            elif module_def['type'] == 'yolo':\n",
    "                # Train phase: get loss\n",
    "                if is_training:\n",
    "                    x, *losses = module[0](x, targets)\n",
    "                    self.pred.append(module[0].output)\n",
    "                    for name, loss in zip(self.loss_names, losses):\n",
    "                        self.losses[name] += loss\n",
    "                    output.append(x)\n",
    "                # Test phase: Get detections\n",
    "                else:\n",
    "                    x = module(x)\n",
    "                    self.pred.append(module[0].output)\n",
    "                    \n",
    "            layer_outputs.append(x)\n",
    "            \n",
    "        self.pred = torch.cat(self.pred,1)\n",
    "\n",
    "        if is_training:\n",
    "            for name in self.loss_names:\n",
    "                self.train_hist[name].append(self.losses[name])\n",
    "            return sum(output)\n",
    "    \n",
    "    def weight_init(self,m):\n",
    "        if type(m) in [nn.Conv2d, nn.ConvTranspose2d, nn.Linear]:\n",
    "            #nn.init.xavier_normal_(m.weight,nn.init.calculate_gain('leaky_relu',param=0.02))\n",
    "            nn.init.kaiming_normal_(m.weight,0.2,nonlinearity='leaky_relu')\n",
    "    \n",
    "    def model_save(self,step):\n",
    "        path = args.model_path + args.model_name +'_Step_' + str(step) + '.pth'\n",
    "        torch.save({ args.model_name :self.state_dict()}, path)\n",
    "        print('Model Saved')\n",
    "        \n",
    "    def load_step_dict(self, step):\n",
    "        \n",
    "        path = args.model_path + args.model_name +'_Step_' + str(step) + '.pth'\n",
    "        self.load_state_dict(torch.load(path, map_location=lambda storage, loc: storage)[args.model_name])\n",
    "        print('Model Loaded')\n",
    " \n",
    "    def plot_all_loss(self,step):\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize= (20,8))\n",
    "        for k in self.train_hist.keys():\n",
    "            plt.plot(self.train_hist[k], label= k)\n",
    "        plt.ylabel('Loss',fontsize=15)\n",
    "        plt.xlabel('Number of Steps',fontsize=15)\n",
    "        plt.title('Loss',fontsize=30,fontweight =\"bold\")\n",
    "        plt.legend(loc = 'upper left')\n",
    "        fig.savefig( args.model_name +\"_Loss_\"+str(step)+\".png\")\n",
    "        \n",
    "    def num_all_params(self,):\n",
    "        return sum([param.nelement() for param in self.parameters()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_d = COCODataset(args.img_path,args.ann_path, transform=to_tensor)\n",
    "training_loader = DataLoader(coco_d,5,shuffle=True)\n",
    "dark = Darknet(Model_defs.model_defs).to(device)\n",
    "\n",
    "epoch = 0\n",
    "all_steps = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(dark.parameters(), lr = args.lr, betas=(args.beta1, args.beta2))\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer,10000,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training \n",
    "dark.train()\n",
    "while epoch < args.n_epoch:\n",
    "    for i, (path, img, target) in enumerate(training_loader): \n",
    "        args.img_p = path # it will be used in both training and detection\n",
    "        start_t = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        img = img.to(device)\n",
    "        target = target.to(device)\n",
    "        #scheduler.step()\n",
    "        loss = dark(img, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        end_t = time.time()\n",
    "\n",
    "        \n",
    "        print('| Step [%d] | lr [%.4f] | Loss: [%.3f] | Time: %.1fs' %\\\n",
    "              ( all_steps, optimizer.param_groups[0]['lr'], loss.item(),\n",
    "               end_t - start_t))\n",
    "        \n",
    "        if all_steps % args.img_show_freq == 0: \n",
    "            # Do IoU compress first\n",
    "            out = non_max_suppression(dark.pred, args.num_classes ,conf_thres=0.5,nms_thres=0.2)\n",
    "            # Plot the first output\n",
    "            if out[0] is not None:\n",
    "                plot_on_o_img([args.img_p[0]], out[0][...,:4].unsqueeze(0), out[0][...,-1].unsqueeze(0),save=True, step= all_steps)\n",
    "            plt.show()\n",
    "            dark.plot_all_loss('Training')\n",
    "            if all_steps % args.model_save_freq == 0: \n",
    "                dark.model_save(all_steps)\n",
    "               \n",
    "        all_steps += 1\n",
    "        if all_steps > 0:\n",
    "            raise StopIteration\n",
    "    epoch +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############### Detection ###############\n",
    "\n",
    "detect_img_root = args.img_path\n",
    "detect_max_objs = 20\n",
    "n_iters = 0\n",
    "save_photo = False\n",
    "\n",
    "detect_dataset = ImageDataset(detect_img_root, to_tensor)\n",
    "detect_dataloader = DataLoader(detect_dataset, 3, shuffle=False)\n",
    "\n",
    "\n",
    "def top_confidence(input_list, max_objs = 50, depth = 7):\n",
    "    from copy import deepcopy\n",
    "    inputs = deepcopy(input_list)\n",
    "    for i in range(len(inputs)):\n",
    "        filled_labels = torch.zeros((max_objs, depth))\n",
    "        if inputs[i] is not None:\n",
    "            filled_labels[range(len(inputs[i]))[:max_objs]] = inputs[i][torch.sort(inputs[i][:,4], descending = True)[1][:max_objs].long()]\n",
    "        inputs[i] = filled_labels\n",
    "    return torch.stack(inputs)\n",
    "\n",
    "        \n",
    "dark.eval()\n",
    "for i, (d_path, d_img) in enumerate(detect_dataloader):\n",
    "    \n",
    "    dark(d_img)\n",
    "    # do IoU compress first\n",
    "    outputs = non_max_suppression(dark.pred, args.num_classes ,conf_thres=0.51,nms_thres=0.05)\n",
    "    outputs = top_confidence(outputs, max_objs = detect_max_objs, depth = 7)\n",
    "    plot_on_o_img(d_path, outputs[...,:4], outputs[...,-1],save= save_photo)\n",
    "    \n",
    "    if n_iters is not None and i >= n_iters:\n",
    "        raise StopIteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_on_o_img(path, target[...,1:], target[...,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
